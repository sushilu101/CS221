{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_path = \"/Users/pranavupadhyayula/PycharmProjects/LipReading/dataset\"\n",
    "# temp_path = \"/Users/pranavupadhyayula/Downloads/cropped_data\"\n",
    "temp_path=\"/Users/sushil/OneDrive/Removable Disk/2Stanford/Spring/CS 221/cropped_data\"\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "# person = [\"F01\", \"F02\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F11\", \"M01\", \"M02\", \"M04\", \"M07\", \"M08\"]\n",
    "person = [\"F01\"]\n",
    "# prob = [\"phrases\", \"words\"]\n",
    "prob = [\"phrases\"]\n",
    "# label = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "label = [\"01\"]\n",
    "instance = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "instance = [\"01\"]\n",
    "min_sequence_length = 4\n",
    "\n",
    "# AVERAGE IMAGES\n",
    "for p1 in person:\n",
    "    for p2 in prob:\n",
    "        for l in label:           \n",
    "            for i in instance:   \n",
    "                X_train = np.zeros((250, 200, 3), dtype=np.float64)\n",
    "                count = 0\n",
    "                curr_path = temp_path + \"/\" + p1 + \"/\" + p2 + \"/\" + l + \"/\" + i\n",
    "                print(curr_path)\n",
    "                \n",
    "                for filename in os.listdir(curr_path):\n",
    "                    if filename.endswith(\".jpg\"): \n",
    "                        count += 1\n",
    "                        image_path = os.path.join(curr_path, filename)\n",
    "                        im = Image.open(image_path)\n",
    "                        X_i = np.array(im.getdata(), dtype=np.float64)\n",
    "                        X_i /= 255\n",
    "                        X_i = X_i.reshape((250, 200, 3))       \n",
    "                        X_train += X_i\n",
    "\n",
    "                X_train /= count\n",
    "                X.append(X_train)\n",
    "                y.append(int(l) - 1)\n",
    "\n",
    "                \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE IMAGES\n",
    "for p1 in person:\n",
    "    for p2 in prob:\n",
    "        for l in label:           \n",
    "            for i in instance:   \n",
    "                curr_path = temp_path + \"/\" + p1 + \"/\" + p2 + \"/\" + l + \"/\" + i\n",
    "                print(curr_path)\n",
    "                \n",
    "                num_img = 0 \n",
    "                img_files = []\n",
    "                for filename in os.listdir(curr_path):\n",
    "                    if filename.endswith(\".jpg\"): \n",
    "                        image_path = os.path.join(curr_path, filename)\n",
    "                        im = Image.open(image_path)\n",
    "                        img_files.append(im)\n",
    "                        num_img += 1\n",
    "                \n",
    "                start_index = (num_img // min_sequence_length) + ((num_img % 4) // 2) - 1 if num_img%2 == 0 else (num_img // min_sequence_length) + ((num_img % 4) // 2)\n",
    "#                 start_index = 0\n",
    "                skip_size = num_img // min_sequence_length\n",
    "#                 skip_size = int(round(num_img * 1.0 / min_sequence_length))\n",
    "                \n",
    "                print(start_index, skip_size, num_img)\n",
    "                \n",
    "                count = 0\n",
    "                X_train = []\n",
    "                for i in range(start_index, num_img, skip_size):\n",
    "                    if (len(X_train) < 4):\n",
    "                        im = img_files[i]\n",
    "                        X_i = np.array(im.getdata(), dtype=np.float64)\n",
    "                        X_i = X_i.reshape((50, 50, 3))\n",
    "                        X_train.append(X_i)\n",
    "                \n",
    "                print(len(X_train))\n",
    "                X_train = np.array(X_train)\n",
    "                X_train = X_train.reshape((min_sequence_length, 50, 50, 3))\n",
    "                X.append(X_train)\n",
    "                y.append(int(l) - 1)\n",
    "                \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN W AVERAGED IMAGES\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', input_shape=(640, 480, 3)))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "## CNN W IMAGE SEQUENCES\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Conv3D(64, kernel_size=2, activation='relu', input_shape=(min_sequence_length, 50, 50, 3), data_format=\"channels_last\"))\n",
    "# # model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None))\n",
    "# # model.add(keras.layers.Conv3D(32, kernel_size=2, activation='relu', data_format=\"channels_last\"))\n",
    "# # model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "## RNN W IMAGE SEQUENCES\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Embedding(input_dim=1300, input_length = 4, output_dim=10))\n",
    "# model.add(keras.layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "## LSTM CNN W IMAGE SEQUENCES\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.Conv3D(64, kernel_size=2, activation='relu', input_shape=(min_sequence_length, 50, 50, 3), data_format=\"channels_last\")))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "# model.add(keras.layers.LSTM(10))\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(min_sequence_length, 50, 50, 3))))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(4, kernel_size=3, activation='relu')))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)))\n",
    "# model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "# model.add(keras.layers.LSTM(10))\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.ConvLSTM2D(16, kernel_size=3, activation='relu', input_shape=(min_sequence_length, 50, 50, 3), data_format='channels_last'))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(keras.layers.ConvLSTM2D(4, kernel_size=3, activation='relu', data_format='channels_last'))\n",
    "# model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "# model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.LSTM(10))\n",
    "# model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.9, test_size=0.1, shuffle= True)\n",
    "model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
    "# model.fit(X, y, epochs=50, validation_data=(X, y))\n",
    "\n",
    "# model.fit(X, y, epochs=2, validation_split=0.2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.applications.vgg16.VGG16(classes=10, weights=None))\n",
    "\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.9, test_size=0.1, shuffle= True)\n",
    "model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X = pickle.load( open( \"data_X.p\", \"rb\" ) )\n",
    "y = pickle.load( open( \"data_Y.p\", \"rb\" ) )\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import pickle\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "X = sequence.pad_sequences(X, maxlen=27)\n",
    "\n",
    "max_x_val = max(X.flatten()) + 1\n",
    "\n",
    "embedding_vecor_length = 4\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_x_val, embedding_vecor_length, input_length=27))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.9, test_size=0.1, shuffle= True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid), batch_size=50)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "person = [\"F01\", \"F02\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F11\", \"M01\", \"M02\", \"M04\", \"M07\", \"M08\"]\n",
    "# person = [\"F01\"]\n",
    "# prob = [\"phrases\", \"words\"]\n",
    "prob = [\"phrases\"]\n",
    "label = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "# label = [\"01\"]\n",
    "instance = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "# instance = [\"01\"]\n",
    "min_sequence_length = 4\n",
    "\n",
    "for p1 in person:\n",
    "    for p2 in prob:\n",
    "        for l in label:           \n",
    "            for i in instance:   \n",
    "                X_train = np.zeros((250, 200, 3), dtype=np.float64)\n",
    "                curr_path = temp_path + \"/\" + p1 + \"/\" + p2 + \"/\" + l + \"/\" + i\n",
    "                for filename in os.listdir(curr_path):\n",
    "                    if filename.endswith(\".jpg\"): \n",
    "                        image_path = os.path.join(curr_path, filename)\n",
    "                        im = Image.open(image_path)\n",
    "                        X_i = np.array(im.getdata(), dtype=np.float64)\n",
    "                        X_i /= 255\n",
    "                        X_i = X_i.reshape((224, 224, 3))       \n",
    "                        X_train += X_i\n",
    "\n",
    "                X_train /= count\n",
    "                X.append(X_train)\n",
    "                y.append(int(l) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from vis.visualization import visualize_cam, visualize_saliency, overlay\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow import keras\n",
    "import matplotlib.cm as cm\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "\n",
    "#gets desired layer\n",
    "last_layer = utils.find_layer_idx(model, \"lstm_1\")\n",
    "\n",
    "#loads images to get CAMs of\n",
    "img1 = utils.load_img(temp_path+'/F07/phrases/01/01/color_007.jpg',target_size=(250, 200, 3))\n",
    "img2 = utils.load_img(temp_path+'/F09/phrases/07/07/color_007.jpg', target_size=(250, 200, 3))\n",
    "print(utils.get_img_shape(img1))\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)\n",
    "#displays original images\n",
    "plt.show()\n",
    "\n",
    "for modifier in [None]:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        grads = visualize_cam(model, layer_idx=last_layer, filter_indices=[4], \n",
    "                              seed_input=img)         \n",
    "        #Overlays the heatmap onto original image.    \n",
    "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
    "        plt.imshow(overlay(grads, img))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = [[13,0,1,1,0,4,0,3,0,4], \n",
    "        [0,11,7,0,1,1,2,3,0,1], \n",
    "        [1,4,18,0,0,1,1,0,0,1], \n",
    "        [0,0,0,15,4,0,3,0,4,0], \n",
    "        [1,0,0,5,10,0,6,0,4,0], \n",
    "        [4,0,0,2,0,9,0,5,0,6], \n",
    "        [2,1,1,5,2,1,10,2,1,1],\n",
    "        [2,0,2,0,0,3,0,16,0,3], \n",
    "        [1,2,1,2,1,1,3,0,14,1],\n",
    "        [1,0,2,1,0,4,0,3,0,15]]\n",
    "columns = ['Stop navigation.',\n",
    "          'Excuse me.',\n",
    "          'I am sorry.',\n",
    "          'Thank you.',\n",
    "          'Good bye.',\n",
    "          'I love this game.',\n",
    "          'Nice to meet you.',\n",
    "          'You are welcome.',\n",
    "          'How are you?',\n",
    "          'Have a good time.']\n",
    "df_cm = pd.DataFrame(array, index = columns,\n",
    "                  columns = columns)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
